'''
    1-  简历deep dive 20min
    2-  基础知识八股文 20min
    3-  做题 两道题 20min
社招一面？
	考察基本技术能力，八股文，写题
	社招：项目深入参与？如何执行一个项目？设计经验？希望工作中不需要培养就能快速上手。

    项目部分
        年限只是参考 更看能力
        一开始考察项目 希望看到和项目比较相关的技术栈的问题？senior/expert
        模块设计的性能问题？ 功能问题？ 
            需求方？
            为什么做？
            解决什么问题？
        =》
            实现过程中的技术问题？
                if 技术难度不高，ask 业务复杂度
                整合依赖方，解决业务复杂度的问题，整体怎么引入模块？
                这个模块如何被拆解？功能性划分？
                瓶颈是什么？SLA（Service Level Agreement）和SLO（Service Level Objective）
                缓存？
        =》
            安于交付

        跨边界？潜力评估	

    团队：
        虚拟机
        k8s集群
        k8s做容器化

    闲扯淡
    shopee 内部转岗新加坡？
        新加坡政府不能批准 EP申请。。。
        shopee（类似TEG） 和 腾讯相比没那么卷，很多从腾讯和阿里过来的。。。

    业务线99%的依赖再infra，纯k8s集群
        shopee（深圳为主 北京 上海，新加坡人力太少？）在外主要是 市场 产品 小游戏支撑团队
        9-7-5 不强制打卡？

    How to drive 
    子模块的设计，这个方案对哪些团队的依赖？如何评估需求性能？可维护性？ 
    presentation 更需要提现自己做了什么？整体团队输入输出是什么？理念是什么？

'''

'''
1-  简历deep dive 20min

inventory service 跨多个云基础架构的服务管理，需要解决的缓存等问题怎么处理？

    多云环境的支持：
    考虑在 Inventory Service 中实现多云环境的支持。这可能涉及到在不同云提供商的基础上构建适应性层，以处理各个云提供商的特定要求。这可以通过使用云相关的SDK、API、标准化的虚拟化技术或容器化来实现。

    缓存策略：
    实施缓存策略以加速库存数据的访问。考虑使用内存缓存（如Redis、Memcached）来存储常用数据，减少对后端存储的频繁请求。这可以显著提高服务的响应时间和可伸缩性。

    数据同步与一致性：
    跨多个云提供商时，确保实现数据同步和一致性。使用适当的数据复制和同步机制，以确保库存数据在不同云环境之间保持一致，避免潜在的数据不一致问题。

    缓存策略设计：
    根据库存数据的使用情况和访问模式，设计合适的缓存策略。可以使用缓存清除策略，例如LRU（最近最少使用）、TTL（生存时间）或事件驱动的清除策略，以确保缓存中的数据保持最新和有效。

    分布式缓存：
    如果库存服务在多个云基础架构中运行，考虑使用分布式缓存系统，以确保高可用性和容错性。技术如Apache ZooKeeper、etcd或Consul可以用于实现分布式协调和缓存管理。

    监控和性能优化：
    实施监控和性能优化策略，以及实时性能分析工具，以追踪库存服务的性能瓶颈和问题。这可以帮助你快速发现和解决缓存问题，确保服务的高可用性和性能。

    安全性：
    跨多个云基础架构时，确保缓存中的敏感数据得到适当的加密和访问控制保护。考虑使用标准的加密方法和身份验证/授权机制来保护缓存数据。

    灾难恢复和备份：
    针对缓存数据，建立灾难恢复和备份策略，以防止数据丢失和提高可用性。定期备份缓存数据，并实施灾难恢复计划，以应对潜在的缓存故障。


怎么审核数据结构？


List 存量数据处理？
处理大量数据的列表（List）通常需要考虑性能、内存占用和处理效率。以下是一些处理大量数据列表的常见策略：

    分批处理：将大量数据分成较小的批次进行处理，而不是一次性处理整个列表。这有助于减少内存占用并提高处理效率。你可以使用循环来遍历每个批次并逐一处理。

    异步处理：如果可能的话，采用异步处理的方式。将数据分成批次后，使用异步处理框架或工具来并行处理这些批次。这可以加速处理大量数据的过程。

    索引和分页：如果你需要按页显示或处理数据，使用索引和分页来限制每次操作的数据量。这对于大型数据列表的分页显示和处理非常有用。

    数据筛选：在处理数据之前，尽量使用筛选条件将数据量减小到最小。这可以通过数据库查询、数据过滤或搜索引擎等方式来实现。减小数据量会提高处理效率。

    数据缓存：如果数据列表是静态或不经常更改的，考虑将数据缓存在内存中，以减少每次请求的数据获取成本。这对于读取密集型操作非常有效。

    数据分区：将数据列表分成多个分区，以便能够并行处理各个分区。这有助于提高处理效率，特别是在分布式环境中。

    数据压缩：如果数据占用大量内存，考虑将数据进行压缩，以减少内存占用。压缩算法可以帮助你减小数据的体积。

    数据流式处理：如果可能的话，采用数据流式处理的方式，不需要一次性加载整个数据列表到内存中。这对于大型数据列表的处理非常有效。

    错误处理：考虑在处理数据时实施错误处理机制，以处理潜在的问题或异常情况，从而不会中断整个处理流程。

    监控和性能调整：在处理大量数据时，定期监控系统的性能，以确保处理过程顺利进行。如果出现性能问题，进行性能调整以优化处理


activity service 发布订阅模型使用什么消息分发中间键？
	kafka


整合？实现过程中如何评估 资源 容量 性能？
	1. 定义性能目标： 首先，明确定义你的系统性能目标，包括吞吐量、延迟、可用性等。这将帮助你确定性能评估的依据。

	2. 基准测试： 在实施消息分发中间件之前，执行基准测试来评估当前系统的性能。这将作为一个基准，用来比较整合后的性能改进。

	3. 容量规划： 根据性能目标和预期的负载，进行容量规划。确定所需的硬件资源、内存、存储和网络带宽等。考虑未来的扩展需求。

	4. 资源配置： 根据容量规划，配置消息分发中间件的资源。这可能包括增加或减少服务器节点、调整内存分配、更改存储配置等。

	5. 性能测试： 在整合消息分发中间件后，进行性能测试。这包括负载测试、压力测试和性能监控。使用性能测试工具和监控工具来评估性能。

	6. 自动化监控： 设置自动化监控和警报系统，以实时监控消息分发中间件的性能。这可以帮助你快速发现潜在问题并采取措施。

	7. 扩展性测试： 模拟高负载情况，测试系统的扩展性。确定在负载增加时如何扩展系统，如何管理集群和负载均衡。

'''



'''
2- 八股文部分

进程 线程 携程？
    进程是独立的执行单位，拥有独立的内存空间，进程间通常需要通过IPC通信。
    线程是进程内的执行单位，共享相同的内存空间，需要小心同步和协调。
    协程是轻量级的线程，可以在同一线程内实现非抢占式多任务，用于高效的并发和异步编程。


redis数据结构 zset有序集合的底层实现
    Redis 中的有序集合（Sorted Set）是一种数据结构，它与普通集合不同之处在于它的每个成员都与一个分数（score）相关联，用于排序成员。
    有序集合通常用于需要按分数进行排序的应用场景，例如排行榜、时间线等。Redis 的有序集合的底层实现采用了跳表（Skip List）和哈希表（Hash Table）的结合。

    有序集合的底层实现主要包括两部分：

        跳表（Skip List）：
            跳表是一种有序数据结构，它通过多层索引来实现快速查找。每个层级的索引都是原始链表的一部分，允许跳过多个元素以快速定位目标元素。
            在有序集合中，每个成员与一个分数关联，并按照分数从低到高排序。跳表的多层索引使得在有序集合中查找特定分数的成员非常高效。
            跳表提供了 O(log n) 的平均时间复杂度用于查找、插入和删除操作。
        哈希表（Hash Table）：
            除了跳表用于快速查找，有序集合还使用哈希表来维护成员到分数的映射关系。
            哈希表的键是成员，值是与之关联的分数。
        当执行插入、删除或更新分数操作时，Redis会同时更新跳表和哈希表，以保持数据的一致性。
    
    通过将跳表和哈希表结合在一起，Redis 的有序集合可以实现高效的按分数排序和查找操作。跳表提供了快速查找的能力，而哈希表维护了成员和分数的映射关系。
    这种组合使得有序集合成为一个非常有用的数据结构，适用于需要排序功能的各种应用场景。


多线程（Multithreading）：

    多线程适用于I/O密集型任务，例如文件操作、网络通信、数据库查询等。因为在I/O操作中线程可能被阻塞，所以多线程可以帮助充分利用CPU时间，执行其他线程的任务。
    Python中的 threading 模块用于实现多线程。你可以创建多个线程来并行执行任务。
    但是，Python 的全局解释锁 (GIL) 会在某些情况下限制多线程程序的并行性。在 CPU 密集型任务中，GIL可能导致多线程的性能不如预期。

多进程（Multiprocessing）：

    多进程适用于CPU密集型任务，例如图像处理、数值计算等，因为每个进程都有独立的Python解释器和GIL。多进程可以充分利用多核CPU，实现真正的并行执行。
    Python 中的 multiprocessing 模块用于实现多进程。你可以创建多个进程来并行执行任务。
    与多线程不同，每个进程有自己的独立内存空间，不会受到GIL的限制。
'''

# 多线程（Multithreading）：
    # 多线程适用于I/O密集型任务，例如文件操作、网络通信、数据库查询等。因为在I/O操作中线程可能被阻塞，所以多线程可以帮助充分利用CPU时间，执行其他线程的任务。
    # Python中的 threading 模块用于实现多线程。你可以创建多个线程来并行执行任务。
    # 但是，Python 的全局解释锁 (GIL) 会在某些情况下限制多线程程序的并行性。在 CPU 密集型任务中，GIL可能导致多线程的性能不如预期。
import threading

def worker():
    # 这里执行任务

# 创建多个线程
thread1 = threading.Thread(target=worker)
thread2 = threading.Thread(target=worker)

# 启动线程
thread1.start()
thread2.start()

# 等待线程结束
thread1.join()
thread2.join()

# 多进程（Multiprocessing）：
    # 多进程适用于CPU密集型任务，例如图像处理、数值计算等，因为每个进程都有独立的Python解释器和GIL。多进程可以充分利用多核CPU，实现真正的并行执行。
    # Python 中的 multiprocessing 模块用于实现多进程。你可以创建多个进程来并行执行任务。
    # 与多线程不同，每个进程有自己的独立内存空间，不会受到 "GIL" 的限制。
from multiprocessing import Process

def worker():
    # 这里执行任务

# 创建多个进程
process1 = Process(target=worker)
process2 = Process(target=worker)

# 启动进程
process1.start()
process2.start()

# 等待进程结束
process1.join()
process2.join()


'''
短连接  VS  长连接

    短连接：
        特点：短连接是指在每次通信完成后立即关闭连接。在每个请求-响应周期中，客户端和服务器都会重新建立连接，通信结束后立即关闭连接。

        频繁连接和关闭：由于每次通信都需要建立和关闭连接，因此会产生频繁的连接和关闭开销。

        资源消耗低：因为连接是短暂的，所以不需要长时间维持连接状态，因此资源消耗较低。

        适用场景：短连接适用于某些特定场景，如一次性请求-响应通信，资源有限的情况，或需要及时释放连接的情况。

    短连接的例子：

        HTTP请求：在典型的HTTP请求中，客户端向服务器发送一个请求，服务器响应后即刻关闭连接。每个HTTP请求-响应周期都是一个短连接。

        SMTP电子邮件传输：SMTP（Simple Mail Transfer Protocol）通常使用短连接来传输电子邮件。客户端建立连接、发送电子邮件，服务器接收后关闭连接。

    长连接：

        特点：长连接是指在多个请求-响应周期之间保持连接的开放状态。客户端和服务器在一次连接建立后可以进行多次请求和响应。

        减少连接开销：由于连接保持开放状态，避免了频繁的连接建立和关闭，从而减少了连接开销。

        资源消耗较高：因为连接需要维持开放状态，可能需要占用更多的资源，包括内存和端口号。

        适用场景：长连接适用于需要频繁通信的场景，如实时聊天应用、在线游戏、流媒体传输等，以减少连接建立和关闭的开销。

    长连接的例子：

        在线聊天应用：在线聊天应用（如即时消息应用）通常使用长连接，使得用户可以保持在线状态，而不需要在每条消息之间建立新的连接。

        WebSockets：WebSockets是一种通信协议，允许客户端和服务器保持长连接，以进行双向实时通信。这在在线游戏、聊天和实时协作应用中很常见。

        数据库连接池：数据库连接池通常维护一组长连接，以便多次查询数据库而无需在每次查询之间重新建立连接。

    总结：
        短连接适用于一次性的请求-响应通信，适用于资源有限或需要及时释放连接的情况。
        长连接适用于需要频繁通信的场景，可以减少连接建立和关闭的开销，但需要更多的资源来维护连接状态。选择短连接还是长连接取决于应用程序的需求和性能目标。
        有时候，也可以在两者之间找到平衡，以兼顾性能和资源消耗。



SLA  SLO是什么？
    SLA（Service Level Agreement）和SLO（Service Level Objective）都是与服务质量和性能管理相关的概念
    通常用于衡量和约定服务提供商与服务消费者之间的服务质量标准。它们有一些区别：

    SLA（Service Level Agreement）：

        SLA 是服务级别协议的缩写，是服务提供商和服务消费者之间的正式合同或协议，明确规定了服务的各种方面，包括性能、可用性、支持等。
        SLA 定义了服务的具体承诺，通常包括指标、报警、处罚和赔偿等方面的详细规定。
        SLA 是一份正式的法律文件，用于确保服务提供商履行其对服务质量的承诺。

    SLO（Service Level Objective）：

        SLO 是服务级别目标的缩写，是服务提供商和服务消费者之间的性能目标或期望，但不一定需要正式合同。
        SLO 定义了服务的期望性能水平，通常包括服务的可用性、延迟、吞吐量等性能指标。
        SLO 是一种更灵活的协议，可以根据需求和服务特性进行调整，而不需要像 SLA 那样受到严格的合同法规限制。
  
    总结一下：

        SLA 是一份正式的合同，明确规定了服务质量和性能的各个方面，通常有法律约束力。
        SLO 是一种性能目标，描述了服务的期望性能水平，更灵活，不一定需要法律合同。
        SLO 可以用作监控和管理工具，以确保服务达到用户的期望，但它不像 SLA 那样具有法律强制力。


tcp 里面的timewait?
    TCP 中的 TIME_WAIT 状态是 TCP 连接的一种状态，它发生在连接的一端（通常是客户端）主动关闭连接后，并等待一段时间
    以确保网络中的任何悬挂数据包都被清除。TIME_WAIT 状态的存在是为了确保数据传输的完整性和连接的正确终结。

        在 TIME_WAIT 状态下，TCP 连接的一端将继续接受来自对方的可能到达的 ACK（确认）数据包，以确保对方已经完全接受到并确认了关闭连接的请求。

    TIME_WAIT 状态的特点包括：

        等待时限：TCP 连接处于 TIME_WAIT 状态的时间限制通常是 2 * MSL（最大报文生存时间），MSL 是报文在网络中能够存活的最长时间。MSL 的典型值是 2 分钟，因此 TIME_WAIT 状态可能持续大约 4 分钟。

        避免数据混淆：TIME_WAIT 状态防止连接中的任何残留数据包干扰下一个与相同连接信息的新连接。

        保证连接结束：通过等待 TIME_WAIT 状态，确保连接双方都已确认连接的关闭，以防止数据丢失或重复传输。

        允许旧数据包的到来：允许在连接关闭后，来自对方的 ACK 数据包仍然到达，以防止在新连接建立时出现问题。

        可用于连接的复用：TIME_WAIT 状态为之后可能建立的具有相同本地和远程端点的连接提供了一些保护。如果新的连接尝试使用相同的端点信息，连接不会混淆或干扰之前的连接。

'''


# python 实现 N级台阶
# 一步或两步完成N级台阶有几种走法

# ChatGPT
# 你可以使用递归或动态规划来实现解决 N 级台阶的走法问题。以下是一个Python示例，使用递归来计算完成 N 级台阶有多少种不同的走法：

def count_ways_to_climb_stairs(N):
    if N <= 1:
        return 1
    return count_ways_to_climb_stairs(N - 1) + count_ways_to_climb_stairs(N - 2)

N = 5  # 例如，要计算 5 级台阶的走法
ways = count_ways_to_climb_stairs(N)
print(f"完成 {N} 级台阶有 {ways} 种不同的走法。")



# 给你一个链表数组，每个链表都已经按升序排列。

# 请你将所有链表合并到一个升序链表中，返回合并后的链表。

# 链表个数K，每个链表长度为N

# 示例 1：

# 输入：lists = [[1,4,5],[1,3,4],[2,6]]

# 输出：[1,1,2,3,4,4,5,6]

# 解释：链表数组如下：

# [

#   1->4->5,

#   1->3->4,

#   2->6

# ]

# 将它们合并到一个有序链表中得到。

# 1->1->2->3->4->4->5->6

# ChatGPT
# 这个问题可以使用分治法和合并两个有序链表的方法来解决。具体步骤如下：

# 定义一个函数 merge_two_lists 用于合并两个有序链表。
# 将链表数组分为左右两部分，然后递归地合并左右两部分。
# 逐渐合并左右两部分，最终得到一个合并后的有序链表。
# 以下是相应的Python代码实现：

# n: max number of element  k: length, 

# time O(k n logk)
# space O(n)

class Node:
    def __init__(self, val=0, next=None):
        self.val = val
        self.next = next

def merge_two_lists(l1, l2):
    # recrsivly merge two lists
    if not l1:
        return l2
    if not l2:
        return l1
    if l1.val < l2.val:
        l1.next = merge_two_lists(l1.next, l2)
        return l1
    else:
        l2.next = merge_two_lists(l1, l2.next)
        return l2

def merge_k_lists(lists):
    if not lists:
        return None
    n = len(lists)

    # merge starting from left
    interval = 1
    while interval < n:
        for i in range(0, n-interval, interval*2):
            lists[i] = merge_two_list(lists[i], lists[i+interval])
        interval *= 2
    
    return lists[0] if n>0 else None

lists = []
lists.append(Node(1,Node(4,5)))
lists.append(Node(1,Node(3,4)))
lists.append(Node(2,6))

print(merge_k_lists(lists))
